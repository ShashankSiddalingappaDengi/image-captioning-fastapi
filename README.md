# ğŸ–¼ï¸ Image Captioning with CNN-LSTM (FastAPI + Docker)

## ğŸ“Œ Overview
This project implements an **Image Captioning API** that generates textual descriptions for input images.  
It combines a **Convolutional Neural Network (CNN)** (ResNet-50) for **feature extraction** and an **LSTM** for **caption generation**.  
The model is exposed as a REST API built with **FastAPI**, containerized using **Docker**, and ready for cloud deployment (AWS EC2 compatible).

---

## âš™ï¸ Tech Stack
- **Python 3.10**
- **PyTorch** (Deep Learning Framework)
- **FastAPI + Uvicorn** (High-performance API framework)
- **Docker** (Containerization)
- **COCO Dataset** (Training and evaluation)
- **NLTK, Pillow, PyCocoTools** (Preprocessing & tokenization)

---

## ğŸ§  Model Architecture
- **Encoder:** CNN (ResNet-50) pre-trained on ImageNet, outputs visual features  
- **Decoder:** LSTM network generating captions word-by-word  
- **Vocabulary Builder:** Uses COCO annotations to tokenize and threshold frequent words  
- **Inference:** Greedy decoding to predict sentence sequences  

---

## ğŸ“‚ Project Structure
image-captioning-fastapi/
â”‚
â”œâ”€â”€ app.py # FastAPI application entrypoint
â”œâ”€â”€ model.py # CNN-LSTM model architecture
â”œâ”€â”€ vocabulary.py # Vocabulary class to tokenize captions
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ Dockerfile # Docker build file
â””â”€â”€ .gitignore


---

## ğŸš€ Run Locally

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/YourUsername/image-captioning-fastapi.git
cd image-captioning-fastapi
```

2ï¸âƒ£ Install dependencies
pip install -r requirements.txt

3ï¸âƒ£ Run FastAPI app
uvicorn app:app --reload


Now open ğŸ‘‰ http://127.0.0.1:8000/docs

Upload an image and get a generated caption instantly.

ğŸ³ Run with Docker
1ï¸âƒ£ Build Docker image
docker build -t image-captioning-app .

2ï¸âƒ£ Run the container
docker run -p 8000:8000 image-captioning-app


Access the API at ğŸ‘‰ http://localhost:8000/docs

ğŸ§© Example Request (Python)
import requests

url = "http://127.0.0.1:8000/predict"
files = {"file": open("example.jpg", "rb")}
response = requests.post(url, files=files)
print(response.json())


Response:

{
  "caption": "a group of people sitting around a table with food"
}

ğŸ§± Vocabulary Generation

To rebuild vocabulary from COCO dataset:

python vocabulary.py


You can adjust:

vocab_threshold

annotations_file path (for COCO captions)



ğŸ“¸ Example Output
Input Image	Predicted Caption
ğŸ§â€â™‚ï¸ Person riding a bike	â€œa man riding a bicycle down a city streetâ€
ğŸ¶ Dog playing in park	â€œa dog running through grass with a ballâ€
